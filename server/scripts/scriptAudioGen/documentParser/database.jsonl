{"id": "6F0A2BEC-B032-11EE-A5B8-F81EFC728A9F.pdf", "text": "Are Language Models More Like Libraries or Like Librarians?\nBibliotechnism, the Novel Reference Problem, and the Attitudes of LLMs\n\nHarvey Lederman\n\nKyle Mahowald\n\nDepartment of Philosophy Department of Linguistics\nThe University of Texas at Austin\n{harvey.lederman,kyle}@utexas.edu\n\nAbstract\n\nAre LLMs cultural technologies like photo-\ncopiers or printing presses, which transmit in-\nformation but cannot create new content? A\nchallenge for this idea, which we call bib-\nliotechnism, is that LLMs often do generate\nentirely novel text. We begin by defending\nbibliotechnism against this challenge, show-\ning how novel text may be meaningful only in\na derivative sense, so that the content of this\ngenerated text depends in an important sense\non the content of original human text. We go\non to present a different, novel challenge for\nbibliotechnism, stemming from examples in\nwhich LLMs generate \u201cnovel reference\u201d, using\nnovel names to refer to novel entities. Such ex-\namples could be smoothly explained if LLMs\nwere not cultural technologies but possessed a\nlimited form of agency (beliefs, desires, and\nintentions). According to interpretationism in\nthe philosophy of mind, a system has beliefs,\ndesires and intentions if and only if its behavior\nis well-explained by the hypothesis that it has\nsuch states. In line with this view, we argue that\ncases of novel reference provide evidence that\nLLMs do in fact have beliefs, desires, and inten-\ntions, and thus have a limited form of agency.\n\n1\n\nIntroduction\n\nDo modern LLMs have beliefs, desires, and inten-\ntions? Over the last few years, this question has\nreceived an enormous amount of attention (e.g.,\nHase et al., 2023; Mahowald et al., 2023; Bender\nand Koller, 2020; Shanahan et al., 2023; Bubeck\net al., 2023). The hypothesis that LLMs do have\nthese states is attractive in part because it offers a\nnatural tool for explaining their behavior. It is stan-\ndard to explain the complex behavior of humans\nand non-human animals in terms of what they think\n(believe), what they want (desire), and what they\nintend. If modern LLMs have beliefs, desires, and\nintentions, that is, if they are agents, we can employ\nthe same explanations of their behavior.\n\nA challenge for those who deny that current\nLLMs are agents in this sense is to provide an\nalternative, equally powerful, explanation of their\nbehavior. The psychologist Alison Gopnik and her\ncoauthors have articulated a striking idea in this\ndirection (Gopnik, 2022b,a; Yiu et al., 2023). In\nGopnik\u2019s view, LLMs are a \u201ccultural technology\u201d,\nlike a library or a printing press. The writer Ted\nChiang also gives voice to an idea in this vein:\n\u201cPrompting it [the LLM] with text is something like\nsearching over a library\u2019s contents for passages that\nare close to the prompt, and sampling from what\nfollows.\u201d (Chiang, 2023). Cosma Shalizi, who\nhas developed this idea in more technical detail\n(Shalizi, 2023), has dubbed the view \u201cGopnikism\u201d.\nBecause we will develop it in our own direction,\nwe call it \u201cbibliotechnism\u201d, combining the Greek\nfor \u201cbook\u201d with the Greek for \u201cskill\u201d. According\nto bibliotechnism, LLMs are not agents; they are\n\u201cjust\u201d cultural technologies, like books and libraries,\nfor processing and querying written text.\n\nCan this view provide an explanation of the\nagent-like behavior of LLMs, which is sufficiently\npowerful to compete with the hypothesis that they\nhave beliefs, desires and intentions? We address\nthis question in a specific application, by examining\nthe meaning-relevant behavior of LLMs (joining\na growing body of work at the intersection of phi-\nlosophy, cognitive science, and NLP; Bender and\nKoller, 2020; Andreas, 2022; Coelho Mollo and\nMilli\u00e8re, 2023; Chalmers, 2023; Mandelkern and\nLinzen, 2023; Piantadosi and Hill, 2022). We ar-\ngue that if LLMs are \u201cjust\u201d a cultural technology\n(and not agents in their own right) then the fact\nthat their outputs refer to certain objects must in an\nimportant sense depend on the fact that their inputs\nrefer to those objects. If LLMs\u2019 reference were\nnot of this \u201cderivative\u201d kind, then there would be\nan important sense in which they were not simply\ntransmitting existing cultural knowledge, but gener-\nating new instances of reference, and perhaps even\n\n4\n2\n0\n2\n\nn\na\nJ\n\n0\n1\n\n]\nL\nC\n.\ns\nc\n[\n\n1\nv\n4\n5\n8\n4\n0\n.\n1\n0\n4\n2\n:\nv\ni\nX\nr\na\n\n \n \n \n \n \n \n\fnew claims.\n\nIn normal cases, text produced by photocopiers\nand printing presses clearly has only derivative\nmeaning, since it is simply a reproduction of\nhuman-generated input. But LLMs often produce\napparently meaningful text, which is entirely novel.\nAt first sight, this fact presents a serious challenge\nfor bibliotechnism: if LLMs\u2019 outputs can only be\nmeaningful by piggybacking on human-generated\noriginals, how could novel text that they generate\nbe meaningful?\n\nWe begin by responding to this challenge for\nbibliotechnism. Using n-grams as a toy model, and\nworking up to more complex modern LLMs, we\nshow how even entirely novel sentences produced\nby LLMs may nevertheless derive their meaning-\nfulness from the meaningfulness of their inputs and\nthus be \u201cderivatively meaningful\u201d.\n\nWe see this as a big step forward for bibliotech-\nnism. But challenges for the proposal remain. Mod-\nern LLMs are not just capable of producing new\nsentences, they can also generate novel reference,\nusing newly invented names apparently to refer to\nnewly created objects. These new names cannot\nderive their reference from original text, since the\nname and its object are not associated in the data (if\nthey occur there at all). We argue that responding\nto this Novel Reference Problem requires compli-\ncating bibliotechnism to such an extent that it calls\ninto question the motivation for doing so.\n\nIn particular, bibliotechnism\u2019s answer to the\nNovel Reference Problem is a worse explanation of\nLLM behavior than the hypothesis that they have\nbeliefs, desires, and intentions. According to many\nviews in the philosophy of mind, this fact provides\nevidence that LLMs do have beliefs, desires, and\nintentions. This is most obviously true given in-\nterpretationism, according to which a system has\nbeliefs, desires and intentions if and only if its be-\nhavior is best explained by the hypothesis that it\nis rational and that it has such states (e.g., Den-\nnett, 1971; Davidson, 1973, 1986; Dennett, 1989;\nMcCarthy, 1979). But the point holds for other\nprominent philosophical views as well. So, the\nproblem of novel reference provides evidence that\nLLMs do have beliefs, desires and intentions, and\nthus are agents of a sort.\n\n2 Prior Work on Meaning in LLMs\n\nA prominent line of argument has suggested that\nLLMs cannot produce reference without being\n\n\u201cgrounded\u201d (e.g., Lake and Murphy, 2023; Bisk\net al., 2020). Perhaps most influentially, Bender\nand Koller (2020) examine the question of whether\nLLMs use language meaningfully. They define\n\u201cmeaning\u201d as a relation between expressions and\ncommunicative intents. They argue that LLMs can-\nnot produce meaningful expressions because they\ncannot have communicative intents concerning ob-\njects they have not had perceptual contact with.\n\nPiantadosi and Hill (2022) respond to this argu-\nment by proposing an alternative account of mean-\ning in which meanings are constituted by the rela-\ntionship among concepts in a particular conceptual\nspace. Since LLMs clearly \u201crepresent\u201d rich infer-\nential relationships as well as relations of semantic\nsimilarity, in their view LLMs can meaningfully\nuse words even without perceptual exposure to their\nreferents.\n\nMandelkern and Linzen (2023) observe impor-\ntant connections between this debate and semantic\nexternalism, a view of meaning which has been\ndominant in the philosophy of language since the\n1980s. On a standard view (Kripke, 1980; Putnam,\n1975; Burge, 1986), people can refer to Shake-\nspeare without having been directly in touch with\nShakespeare, by belonging to a community whose\noverall use of this word stands in an appropriate\ncausal relationship to the poet. Mandelkern and\nLinzen accordingly argue that whether LLMs can\nrefer to Shakespeare comes down to whether LLMs\n\u201cbelong to our speech community\u201d (cf. Ostertag,\n2023).\n\nCoelho Mollo and Milli\u00e8re (2023) argue that\nLLMs achieve the capacity to refer through rein-\nforcement learning with human feedback (RLHF)\n(or possibly during zero-shot learning). They sug-\ngest that reference (what they call \u201creferential\ngrounding\u201d) can only be achieved if there is a rele-\nvant normative standard which connects the LLM\u2019s\nusage to the world. As a result they think that\ngrounding is achieved for current LLMs (essen-\ntially) if and only if there is RLHF, since in their\nview it is only in this process that the human train-\ners appropriately transmit a normative standard,\ndirected at the truth, to the LLMs.\n\nThese authors do not consider how questions\nabout the meaningfulness of LLM-generated text\nrelate to bibliotechnism. They also do not address\nhow, if at all, simpler models like n-gram models\ncan produce meaningful text. In fact, to the ex-\ntent that they deliver verdicts about n-grams, their\ntheories do not clearly imply that n-grams can pro-\n\n\fduce meaningful text. By contrast, we will argue\nthat n-gram models can clearly produce meaning-\nful words. We will build on this account in the case\nof n-grams to offer a novel extension of bibliotech-\nnism, showing how the view can accommodate the\nmeaningfulness of entirely novel text generated by\nLLMs. We then introduce the \u201cNovel Reference\nProblem\u201d, a new challenge for bibliotechnism.\n\n3 Background\n\nGenerative language models, \u201clarge\u201d or other-\nwise, are given as input PrimaryData.\nThe\nPrimaryData typically includes a corpus (in the\ncase of LLMs, essentially the whole internet) that\nthe model is trained on, along with a (usually)\nhuman-generated prompt given at generation time.\nThe model is then sampled to probabilistically pro-\nduce GeneratedText.\n\nWe will assume that PrimaryData is text cre-\nated by humans, as is the prompt (setting aside the\nfact that, in practice, massive corpora likely con-\ntain automatically generated text). And we will\ntake it as uncontroversial that PrimaryData refers\nto things in the world. For instance, if a human-\nauthored biography of Shakespeare is included in\nPrimaryData and includes the line \u201cShakespeare\nwas born in 1564.\u201d, it is referring to the poet.\n\nOur arguments apply both to models trained\npurely on a word prediction task, and to those\nthat use more modern augmentation techniques\nlike RLHF. What matters for our purposes is that\nthe models are (a) trained on largely naturalistic\nhuman data to generate text, (b) produce largely\ngrammatical and intelligible content, and (c) do not\nsimply verbatim reproduce their training data. To-\nday\u2019s LLMs (e.g., OpenAI\u2019s ChatGPT, Anthropic\u2019s\nClaude, Meta\u2019s LLaMa) have these properties: they\nare trained on human data, produce grammatical\nand fluent content (even if they sometimes halluci-\nnate), and generate at least some novel content as\nmeasured by n-gram overlap (McCoy et al., 2023).\nWe focus on purely text-based models, but much\nof the argument could be easily extended to multi-\nmodal models that include visual input or output.\nThree points about philosophical terminology\nwill be important. Philosophers often distinguish\nbetween \u201creference\u201d and \u201cmeaning\u201d. As we will\nuse the terms, any expression that refers has a mean-\ning, although many meaningful expressions do not\nrefer. For simplicity, the only words that we take\nto refer are meaningful common and proper nouns.\n\nIt is uncontroversial that a normal use of the word\n\u201cShakespeare\u201d refers to Shakespeare (and is mean-\ningful). By contrast, in our (stipulative) usage, nor-\nmal uses of the expression \u201cwas born\u201d is meaning-\nful, but it does not refer.\n\nSecond, there is a difference between the word\n\u201cShakespeare\u201d and particular inscriptions or tokens\nof this word. If the word \u201cShakespeare\u201d is written\non a blackboard five times, there are five inscrip-\ntions of this one word on the blackboard. We as-\nsume inscriptions of words can refer and be mean-\ningful.\n\nThird, and finally, we will distinguish between\n\u201creferring\u201d that is done by an agent (\u201cIn his indirect\nway, Marlowe was referring to Queen Elizabeth.\u201d),\nand referring that is done by particular inscriptions\nof words. Since our goal is to explore a view on\nwhich LLMs are not agents, we will be investi-\ngating the question of whether they can produce\ninscriptions which refer and are meaningful. We\nwill not be assuming that they themselves can re-\nfer.1\n\n4 From Cultural Technology to\n\nDerivative Reference and Meaning\n\nOur first main claim is that bibliotechnism implies\nthat LLMs produce inscriptions which have mean-\ning (and refer), if at all, only derivatively.\n\nGopnik and other bibliotechnists understand cul-\ntural technologies, like books and libraries, as tools\nfor the transmission and dissemination of informa-\ntion, allowing the accumulation of knowledge over\nlarge stretches of space, and, most notably, time.\nThese technologies are, crucially, not themselves\nresponsible for new ideas or information.\n\nThese technologies transmit information by rely-\ning on what we will call derivative meaning and ref-\nerence. When a biographer writes the word \u201cShake-\nspeare\u201d, their inscription of the word refers to the\npoet. As a result of this initial case of reference,\nthe inscription of \u201cShakespeare\u201d on the 131st page\nof the 1004th copy of the 3rd printing of this biog-\nraphy, also refers to the poet. The same holds also\nfor inscriptions of \u201cShakespeare\u201d on photocopies\n\n1Mandelkern and Linzen (2023) often move without com-\nment between the question of whether a particular agent refers,\nand the question of whether particular inscriptions of words\nrefer. But this distinction seems to us of key importance here,\nsince it may take beliefs and desires to refer as an agent, but\nit does not take such attitudes to produce inscriptions which\nrefer (as we will argue in a moment, photocopiers can do so,\nas can n-grams).\n\n\fof this page of this edition of the book, even if the\nphotocopies are produced by accident.\n\nA similar thesis applies not just to the refer-\nence of expressions like \u201cShakespeare\u201d but also\nto the meaning of complex expressions (involving\nmore than one word) like \u201cShakespeare was born\nin 1564\u201d.\n\nWe will say that the original inscriptions, which\nwere created by the author immediately, are in-\nstances of basic reference and meaning, while the\nother inscriptions are instances of derivative refer-\nence and meaning. We stipulate, as part of the defi-\nnition of these terms, that it is only agents (which\nwe understand to mean entities with beliefs, desires,\nand intentions) who produce inscriptions which re-\nfer or are meaningful basically. Beyond this stipu-\nlation, the distinction between basic and derivative\nreference and meaning is rough, but we will only\ndeal with clear examples of each category in what\nfollows.\n\nAccording to bibliotechnism, LLMs are not\nagents. So, according to bibliotechnism, they can\nonly produce inscriptions which refer or are mean-\ningful derivatively.\n\nIs this consequence of the position correct? We\nwill examine this question in stages. We first argue\nthat unigram models can produce words which refer\nand are meaningful derivatively, but that they can-\nnot produce complex expressions which are deriva-\ntively meaningful. We then explain how, going\nbeyond unigrams, LLMs can produce complex ex-\npressions, including longer stretches of entirely\nnovel text, which is derivatively meaningful. We\nfinally turn to the question of whether this is the\nonly way that LLM-produced expressions can be\nmeaningful, and provide a new challenge to the\nidea that it is.\n\n5 Causal Connection and Derivative\nReference: The Case of N-grams\n\nIn this section, we argue that derivative reference\nand meaning can be achieved by an appropri-\nate causal connection between PrimaryData and\nGeneratedText, and show how this vindicates the\nidea that n-grams can produce derivatively mean-\ningful words.\n\nSince the 1970s, philosophers have developed\nthe idea that causal connection can play a key role\nin facilitating reference, and that our ability to refer\nto (say) Shakespeare is partly explained by there\nbeing an extended causal chain, tracing from cur-\n\nrent humans, through their teachers, their teach-\ners\u2019 teachers, and so on, all the way back to the\npoet (Kripke, 1980; Geach, 1969; Donnellan, 1970;\nEvans, 1973).\n\nWe suggest that, analogously, derivative refer-\nence and meaning depend on an appropriate causal\nchain tracing from a new inscription back to an\n\u201coriginal\u201d. It is because the inscription of \u201cShake-\nspeare\u201d on the 131st page of the 1004th copy of\nthe 3rd printing of the biography, is appropriately\ncausally connected to the original inscription writ-\nten by the author of the biography, that it refers to\nthe poet. The same holds also for inscriptions of\n\u201cShakespeare\u201d on photocopies of this page: these\nnew inscriptions can refer because they are appro-\npriately causally connected to the original.\n\nThis \u201cappropriate\u201d causal connection does not\nrequire human supervision. If a page falls out of\nits binding, and flies into a photocopier which is\nmalfunctioning, making copies by accident, the\ninscriptions on the resulting page would still refer\nto the poet. If whole sentences are copied by the\nmachine, these sentences would also be derivatively\nmeaningful, because of their causal connection to\nthe original inscription.\n\nThis observation already shows that large-n n-\ngram models (with n sufficiently large, e.g. 1000,\nthat they copy long stretches of their input) can\nproduce meaningful inscriptions. Since such an\nn-gram model copies their input, their output can\nbe meaningful, just as the photocopier\u2019s is.\n\nMatters are less straightforward for unigram\nmodels, which sample from a distribution of single\nwords. We can think of this model as implemented\nby taking all of its PrimaryData, choosing word-\ninscriptions from the PrimaryData at random, and\nthen copying the chosen inscription. The inscrip-\ntions the n-gram then produces are again just like\nthose of a copier (or of the large-n model): they\nhave a direct causal connection to the original in-\nscriptions. As a result, if the model produces an\ninscription of \u201cShakespeare\u201d, this inscription will\nbe meaningful (and refer) derivatively, piggyback-\ning on the meaning and reference of the original\ninscription of this word.\n\nIn this case, however,\n\nthere is a new phe-\nnomenon, not exhibited in the case of the photo-\ncopier or large-n n-gram. Each of the inscriptions\nof the individual words produced by the unigram\nwill be meaningful (and possibly refer), but it does\nnot seem that inscriptions of complex expressions\nformed from these words will be. The vast ma-\n\n\fjority of the time, the string of words the model\nproduces will be gibberish, and uncontroversially\nmeaningless. At low odds the unigram model will\nproduce a \u201creasonable\u201d string like \u201cShakespeare\nwas born in 1564\u201d. With even lower odds, such\na reasonable string will be produced as an exact\ncopy of an original string. But even in these latter\ncases, the fact that a meaningful string (or even a\ncopy) is produced is a fluke, a complete accident.\nSince the model produces \u201cappropriate\u201d expres-\nsions in such a chaotic way, we judge that, even\nwhen the model produces an inscription of a string\nthat would be meaningful if produced by a human\nin a normal way, this inscription does not have a\nmeaning. The generated text is not appropriately\ncausally connected to the PrimaryData to inherit\n\u201cglue\u201d that binds the complex expression together.\nOne way to put this point would be to say that,\nwhile the inscription it produces may look like an\ninscription of a sentence (and a human who sees\nit may be able to conjure up a meaning associated\nwith it), it is not really an inscription of a sentence,\nbut just an inscription of words which could (in\ndifferent circumstances) have made up a sentence.\nThese inscriptions are like words blown together\nby the wind, or sand dunes blown into the shape of\na sentence\u2014or like Mandelkern and Linzen\u2019s \u201cants\nformicating meaninglessly in the sand\u201d.\n\n6 LLMs do Produce Derivatively\n\nMeaningful Complex Expressions\n\nTo this point we have seen how inscriptions of com-\nplex expressions can be derivatively meaningful if\nthey are copied from PrimaryData. But modern\nLLMs often produce text which has never been\nseen before in their PrimaryData (McCoy et al.,\n2023). Can bibliotechnism accommodate the mean-\ningfulness of such novel text?\n\nWe will argue that it can, by arguing that there\nis another route to producing derivatively meaning-\nful inscriptions of complex expressions, where the\ncausal chain which traces from the inscription of\nindividual words back to the original inscriptions\nis distinct from the causal chain which is responsi-\nble for the expression-level features which make a\nwhole complex expression meaningful.\n\nTo see the basic idea, suppose we have a rudi-\nmentary model, which, when fed some text, finds\nany inscriptions of names in this sentence (search-\ning on the basis of a database) and then replaces\neach of these names uniformly with a name drawn\n\nat random from the distribution of all names in\nits PrimaryData. It seems plausible that in this\ncase, the model would produce not just individual\nwords which are derivatively meaningful, but a new\nsentence which would, as a whole, be derivatively\nmeaningful. For instance, if we gave this model\nour Shakespeare sentence, and it produced \u201cBarack\nObama was born in 1564\u201d this inscription would\nbe false, but meaningful. Here, the causal history\nof the context and the causal history of the indi-\nvidual name printed are different, but the whole\nexpression would still be derivatively meaningful.\nPlausibly, this is because the operation as a whole\nis causally sensitive to sentence-level features, in\nsuch a way as to reliably produce an intelligible\nsentence. This could be so, even if the resulting\nsentence has never been seen before.\n\nThis example shows that even novel text can be\nderivatively meaningful. It also suggests that, if\nLLMs are causally sensitive to high-level features\nof their PrimaryData, in such a way as to transmit\nthose features to their GeneratedText, it is possi-\nble that even their entirely novel GeneratedText\ncould be derivatively meaningful. We suggest that\none such high-level feature is intelligibility. As\nwe will understand the notion, intelligibility of\nexpressions requires they they be at least quasi-\ngrammatical (sentences with minor grammatical\nerrors are often perfectly intelligible), but it re-\nquires more than just grammaticality, since not all\ngrammatical sentences are meaningful (and hence\nnot intelligible). If LLMs are causally sensitive\nto intelligibility, so that they produce intelligible\noutputs from intelligible inputs, then intelligible\ncomplex expressions in their GeneratedText will\nbe derivatively meaningful, with individual words\ninheriting the meanings of the original inscriptions\nfrom which they were \u201ccopied\u201d, and whole expres-\nsions inheriting the \u201cglue\u201d of intelligibility from\nthe PrimaryData.\n\nThere is strong evidence that modern LLMs are\nin fact appropriately causally sensitive to the intel-\nligibility of their PrimaryData. First, they over-\nwhelmingly produce text which is intelligible to\nhuman users. This does not prove that they are\ncausally sensitive to this property, but it is strong\nevidence that they are. Second, it seems extremely\nplausible that if LLMs were trained on gibberish,\nthey would output gibberish. These two claims\nat least point toward the verdict that they causally\ntransmit the intelligibility of their data.\n\nGiven the state of text generation even 5 or 10\n\n\fyears ago, we think this is a surprising fact. But it\ndoes seem a fact. And, as a consequence of this fact,\nthere is a clear story according to which modern\nLLMs do not just produce derivatively meaningful\nsingle words like unigrams, but in fact can pro-\nduce derivatively meaningful complex sentences\nlike \u201cShakespeare was born in 1564\u201d.\n\nIntelligibility in our sense does not require truth\nor even sufficiently reliable production of the\ntruth. False sentences like \u201cShakespeare was born\nin 2023\u201d are intelligible. This is important be-\ncause even the best LLMs at the time of writ-\ning are known to confabulate or fabricate infor-\nmation. But getting a fact wrong (e.g., saying\nShakespeare was born in 2023 instead of 1564) is\nimportantly different than if the model produces in-\ncoherent responses. If an LLM reliably responded\nto queries about Shakespeare\u2019s birth with gibber-\nish, this would at least be some evidence that it is\nnot in fact causally sensitive to the intelligibility of\nits data in such a way as to generate derivatively\nmeaningful complex expressions.\n\nIt is instructive to compare LLM-generated text\nto text generated by bigram or trigram models. Bi-\ngram and trigram models are trickier cases than uni-\ngrams because they do copy short complex phrases\nand might be statistically likely to combine them in\nways that are more plausibly meaningful as a whole.\nFor instance, a bigram model that outputs \u201cShake-\nspeare wrote plays\u201d in a way \u201cknows\u201d that \u201cwrote\u201d\nis a likely continuation for \u201cShakespeare\u201d and that\n\u201cplays\u201d is a likely continuation for \u201dwrote\u201d. But, be-\nsides the one mediated by the verb \u201cwrote\u201d, there is\nno causal connection between \u201cShakespeare\u201d and\n\u201cplays\u201d. So, even when the model produces strings\nof sentence-length that are grammatical, and even\nwhen individual phrases may be judged meaning-\nful, it seems that, as with unigram models, longer\nsentences should probably not be understood as\nmeaningful (although this is more of a borderline\ncase): they lack the straightforward \u201ccopy property\u201d\nof higher-n n-gram models but also are not causally\nsensitive to intelligibility as modern LLMs are.\n\nWe conclude that LLMs can produce novel\ntext which is nevertheless derivatively meaning-\nful, because they copy individual tokens from their\nPrimaryData, and assemble them in ways that are\ncausally sensitive to the high-level feature of intel-\nligibility in their PrimaryData.\n\nBefore closing this discussion, we want to offer\none important clarification about the basis of our\njudgment that unigrams do not produce derivatively\n\nmeaningful complex expressions. The basis for\nthis judgment is not the fact that n-grams are only\ntrained on words. It is instead because this training\nmechanism does not lead to causal sensitivity to\nrelevant high-level features of their PrimaryData.\nTo put this another way: we are not interested in a\nnarrow form of \u201cinput-sensitivity\u201d, but instead in a\nbroader notion of causal sensitivity.\n\nThis contrast can be illustrated by considering\nagain a photocopier. The fact that a photocopier\nresponds to (say) one or another aspect of the ink\nused to write original letters is irrelevant to the\nquestion of whether the tokens it produces refer\u2014\nas long as this underlying low-level mechanism\nreliably produces inscriptions of actual words, that\nis, as long as the low-level mechanism leads to\ncausal sensitivity to the right high-level features.\n\nThe same point can be made in connection to\nan n-gram trained not on word-frequency but on\nletter-frequency. In fact, a unigram model trained\non letters (as opposed to words) with the same\nPrimaryData as the models above, would in its\ntrained form do nothing more than spit out let-\nters randomly in proportion to their frequency in\nPrimaryData. But if (per impossibile) the letter-\ntrained unigram somehow were sufficiently reli-\nable in producing real words (as a 10-gram model\ntrained over letters might be), that would be evi-\ndence that it was sensitive to the fact that letters\nin its PrimaryData formed words, and that it was\nproducing derivatively meaningful inscriptions. In\nshort, an n-gram trained on letters may fail to pro-\nduce referring inscriptions not because it is trained\non the letters, but because that training mechanism\n(as a matter of fact) is not causally sensitive to the\nright high-level features of its PrimaryData.\n\nThis concludes our response to the first challenge\nfor bibliotechnism, that LLMs can produce novel\ntext which is apparently meaningful. We next turn\nto a new and different kind of challenge to this view:\nthe fact that LLMs can generate novel reference.\n\n7 The Novel Reference Problem: LLMs\nDo Not Produce Only Derivatively\nMeaningful Expressions\n\nWe will illustrate the problem of novel reference\nwith two examples.\n\nFirst, LLMs can produce tokens of names they\nhave never seen before, intuitively in such a way\nthat they refer to previously referred-to objects.\nSuppose we ask an LLM to choose any real histor-\n\n\fical figure it likes, and then come up with a new\nname and tell us facts about this historical figure.\nChatGPT (GPT-4) completed this task by telling\nus about \u201cMarion Starlight\u201d, a figure \u201cborn in the\n18th century\u201d, who \u201cauthored a famous pamphlet\nthat criticized the French monarchy and advocated\nfor the rights of the third estate\u201d, \u201cplayed a critical\nrole in the French Revolution\u201d, \u201cbecame increas-\ningly paranoid and was involved in the Committee\nof Public Safety, which oversaw the Reign of Ter-\nror\u201d, and \u201cwas arrested and executed during the\nThermidorian Reaction, which marked a turning\npoint in the Revolution.\u201d We think it is clear that\ninscriptions of \u201cMarion Starlight\u201d in this text refer\nto the historical figure Robespierre.\n\nBut nothing in PrimaryData (presumably) as-\nsociates Marion Starlight with Robespierre. So\nthe LLM\u2019s inscriptions of this name cannot re-\nfer to Robespierre in virtue of original reference\nexhibited by inscriptions of this name in the\nPrimaryData.\n\nA second example sharpens the problem. Sup-\npose we ask an LLM to produce the TikZ codes for\na series of pictures which it has never seen before,\nto give elements of those pictures names, and then\nto describe features the picture would have when\ntypeset using those names. If an LLM can do this,\nthen it is even more clear than in the previous ex-\nample that the reference of these expressions could\nnot be due to some reference in the PrimaryData,\nsince the object did not exist in this form until the\nLLM created it (provided the picture really is new).\n\nWhile we do not here present empirical experi-\nments for these cases and rely on anecdotal exam-\nples, we think they are well within the capabilities\nof modern LLMs, which have been empirically\nshown to be able to generate, refer to, and manipu-\nlate elements of code-generated pictures (Bubeck\net al., 2023) and refer meaningfully to novel orien-\ntations of elements in visual and color spaces (Patel\nand Pavlick, 2021).\n\nThese examples cannot be straightforwardly ac-\ncommodated by the derivative reference account\ngiven so far, since there is no association between\nthe names and their referents in PrimaryData. In\nthe next section we consider some responses to\nthis problem which involve expanding the notion\nof derivative reference, before turning to our own\nfavored response in the conclusion.\n\n8 Responses to the Novel Reference\n\nProblem\n\nIf bibliotechnism is correct, our cases of \u201cnovel\u201d\nreference must in fact be derivative, so there must\nbe some way in which the inscriptions \u201cpiggyback\u201d\non original human reference. Other than the orig-\ninal data, which we have already ruled out, there\nseem to be four other salient places where human\nattitudes might enter the model pipeline. In this\nsection, we briefly consider some responses to the\nproblem of novel reference based on these four\npossibilities.\n\nHuman Feedback in RLHF A first point at\nwhich human intentions might enter the pipeline\nis during the RLHF step, which Coelho Mollo and\nMilli\u00e8re (2023) claim as a central point. Human\nintentions may ground LLM reference by \u201calign-\ning\u201d the LLM with human goals (Bai et al., 2022;\nBommasani et al., 2021). While RLHF clearly\ninfluences model capabilities, even models with-\nout RLHF are able to produce intelligible output\nand follow instructions to some extent. Thus, an\naccount which considers RLHF-ed models to be\nradically different in their basic referential abilities\nseems not to align with the empirical data.\n\nCreators\u2019 Intentions A second point at which\nintentions might enter the pipeline is during the\ncreation of the LLM. A very precise thermometer\nmay report a temperature no one has ever thought\nabout, and in doing so it seems to \u201crefer\u201d to this\ntemperature. Its ability to do this seems to derive\nfrom a human\u2019s general intention at the time of\nconstruction: that any indication using some num-\nbers would count as a temperature. By the same\nlogic, one might say that LLMs\u2019 creators\u2019 inten-\ntions might be general enough to guarantee that\nthe words it produces would be meaningful in their\nrespective languages and perhaps to accommodate\nour cases of novel reference.\n\nEven supposing this response were to offer an\nexplanation of the capacity for novel reference in\nLLMs as they are today (we have our doubts), this\napproach is not sufficiently general to accommo-\ndate our judgments about LLM meaning in closely\nrelated cases. LLMs can be created for different\nreasons: if the \u201csame\u201d LLM was created by Team A\nfor the purpose of measuring sentence probabilities\nfor use in a downstream application, and by Team\nB for use as a chatbot, it seems odd to conclude that\nonly the second of these can generate meaningful\n\n\ftext in our cases.\n\n9 Conclusion\n\nIntentions in Generating the Prompt A third\npoint at which human intentions might emerge is\nthrough the user. Someone might say that in our\nparticular prompts involving novel reference, the\nuser has an intention that whatever name the LLM\nproduces (e.g, Marion Starlight) should refer to\nthe person best described by the surrounding text\n(or to the aspect of the diagram best described by\nthis text). On this view, the user (in writing the\nprompt and interpreting the output) is crucial for\ngenerating meaning, and the LLM\u2019s words are only\nmeaningful in virtue of user attitudes.\n\nThis approach again does not make correct pre-\ndictions in relevantly similar cases. Suppose that\nwe generate prompts randomly and provide them\nto an LLM (perhaps generating them by a unigram\nmodel), and that by chance a model is fed the\nprompt asking for a story featuring a new name\nfor an historical figure. If the LLM offered the\nresponse above, it still seems to us that the LLM\nwould produce inscriptions which refer to Robe-\nspierre. But this reference would not be due to the\ncreator of the prompt, since by assumption there is\nno user which has intentions.\n\nReader\u2019s Intentions A fourth and final place\nwhere human intentions might enter the picture is\nthrough the reader of the text (who might not be the\ncreator of the prompt). In this vein, Cappelen and\nDever (2021, Ch. 4) develop a receiver-focused\n\u201cmetametasemantics\u201d according to which tokens\ncan count as meaningful in virtue of how readers\nwould understand them. We consider this response\nthe most promising option for bibliotechnists, and\nit deserves much more detailed discussion than we\ncan give here.\n\nHere we just mention one preliminary reserva-\ntion, as an indication of a direction for future work.\nAs it stands the theory cannot obviously distin-\nguish between cases that are equally intelligible to\na reader but intuitively differ in meaning. For in-\nstance, the same string that would be meaningful if\nan inscription of it was generated by a person will\nnot be meaningful if is created by the wind in the\nsand. But the string will not differ in intelligibility\nto a reader in its two inscriptions. If the theory is to\nsave bibliotechnism, it must draw this distinction\nwithout appealing to differences in the attitudes of\nthe producers of the relevant text. This may not be\nimpossible to do, but it is a challenge for the view\nas it stands.\n\nWe argued that bibliotechnism requires that LLMs\nproduce inscriptions which are only derivatively\nmeaningful. We went on to develop a notion of\nderivative meaning which allows that many inscrip-\ntions, even of complex expressions, produced by\nLLMs are in fact derivatively meaningful. But we\nargued that the problem of novel reference poses a\nchallenge for the view that all of them are.\n\nThroughout, we have focused on this notion of\nderivative meaning. Some proponents of a view\nsimilar to bibliotechnism might prefer to develop\nin a different way. There is a sense in which the\npresence of smoke \u201cmeans\u201d that there is fire, and\nGrice (1957) called this sense of \u201cmeaning\u201d, \u201cnat-\nural meaning\u201d (as opposed to linguistic meaning).\nWe think it would be interesting to see how a ver-\nsion of bibliotechnism would look if developed\nusing natural meaning instead of linguistic mean-\ning. We have not taken this route here because we\nhave not been able to come up with a reasonable\nexact proposal for what the \u201cfire\u201d would be that the\nLLM text indicates as the \u201csmoke\u201d. We also note\nthat, even if this view were to be developed in more\ndetail, it will still face the novel reference problem,\nsince it is highly unclear what \u201cfire\u201d the LLM is\nindicating in those cases.\n\nIn closing, we want to consider more generally\nhow our discussion of biblotechnism and novel\nreference may contribute to the broader question\nof whether LLMs have attitudes like belief, desire,\nand intention.\n\nLet us start with the place of these attitudes in the\nexplanation of human behavior. Human behavior\ncan presumably be explained and predicted at the\nmicrophysical level. But the fact that it can be\ndoes not mean that beliefs, desires and intentions\nare not also useful in explaining and predicting\nbehavior. As we all know from our daily lives, they\nare extremely useful for these purposes.\n\nThe examples of novel reference provide an ex-\nample where it is easier to explain LLMs\u2019 behavior\nby attributing beliefs, desires, and intentions to\nthem, rather than by offering a complex, contorted\ntheory of derivative reference. For instance, our\nfirst case can be explained by the hypothesis that\nthe LLM intends for \u201cMarion Starlight\u201d to be equiv-\nalent to \u201cRobespierre\u201d (among many other possible\nexplanations).\n\nAccording to the prominent tradition of \u201cinter-\npretationism\u201d in philosophy and cognitive science\n\n\f(e.g., Dennett, 1971; Davidson, 1973, 1986; Den-\nnett, 1989), (roughly) a system has beliefs, desires\nand intentions if and only if its behavior is best ex-\nplained by the hypothesis that it has those attitudes\nand is rational. Along these lines, McCarthy (1979)\nwrites: \u201cTo ascribe certain beliefs, knowledge, free\nwill, intentions, consciousness, abilities or wants\nto a machine or computer program is legitimate\nwhen such an ascription expresses the same infor-\nmation about the machine that it expresses about\na person. It is useful when the ascription helps us\nunderstand the structure of the machine, its past or\nfuture behavior, or how to repair or improve it. \u201d\nHe notes that this is most usefully applied to ma-\nchines whose inner workings are opaque, although\nit is more straightforwardly (but less usefully) ap-\nplied to transparent machines like thermostats. In\nour view, we can explain cases of novel reference\nin a much more straightforward way if we attribute\nsome representational states (and possibly beliefs,\ndesires and intentions) to the LLMs. According to\ninterpretationism, this fact provides strong, straight-\nforward evidence that LLMs do have beliefs, de-\nsires and intentions. But many other philosophical\nviews of these states, including varieties of func-\ntionalism, will also take the fact that attributing\nthese states to LLMs provides a good explanation\nof their behavior, to be evidence that they have\nthese states (see, e.g., Schwitzgebel, 2023; Gold-\nstein and Kirk-Giannini, manuscript). We cannot\noffer a comprehensive survey of approaches to be-\nlief, desire, and intention here, but this is a reason-\nably feature of such approaches, even if it is not\nuniversal.\n\nAs we emphasized earlier, the explanation of\nLLM behavior in terms of beliefs, desires and in-\ntentions, is not meant to replace an explanation of\ntheir behavior at a finer level of detail. Of course\nthe LLM is also \u201cjust\u201d sampling from a distribution\nover words, just as humans are presumably \u201cjust\u201d\ncollections of atoms. But at a high-level the LLM\nbehavior may also be well explained (indeed, bet-\nter explained) by its having some representational\nstates.\n\nTo say that LLMs have beliefs, desires and inten-\ntions would not be to say that human or superhu-\nman intelligence is just around the corner (contra\nBubeck et al., 2023). Spiders, rabbits, and possibly\neven fish have beliefs, desires, and intentions. But\nthese animals are not super-intelligent.\n\nOur conclusions here are in line with a growing\nbody of work which advocates using tools from\n\ncognitive science to understand LLMs (Mitchell\nand Krakauer, 2023), perhaps viewing them as\nalien intelligences (Frank, 2023) or as role play-\ners (Shanahan et al., 2023) to be studied from the\noutside. In our view, the question of whether LLMs\nhave representational states, and what representa-\ntional states they have will be settled by careful\nanalysis of a wide array of their behavior and how\nit can best be explained, leading to a holistic case\nthat they do or do not have these states. If theories\nlike bibliotechnism which do not attribute repre-\nsentational states to LLMs can only explain the\nbehavior of LLMs by becoming thinner and more\ncomplex, a simpler, stronger explanation involving\nrepresentational states becomes more attractive. By\nputting this kind of pressure on such alternative ex-\nplanations, the novel reference problem provides\nsome evidence that LLMs do have such represen-\ntational states and, accordingly, at least a limited\nform of agency.\n\n10 Acknowledgments\n\nFor helpful comments on drafts, we thank Josh\nDever, Robbie Kubala, Matt Mandelkern, Gary Os-\ntertag, and Sinan Dogramaci (who introduced the\ncreators\u2019 intentions objection, comparing LLMs\nto thermometers). For helpful conversations in\nthinking through these issues, we thank David\nBeaver, Ray Buchanan, Chiara Damiolini, Katrin\nErk, Steven Gross, Dan Harris, and participants\nin UT Austin\u2019s LIN 393 graduate seminar. K.M.\nacknowledges funding from NSF Grant 2139005.\n\nReferences\n\nJacob Andreas. 2022. Language models as agent mod-\nels. In Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 5769\u20135779, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\n\nYuntao Bai, Saurav Kadavath, Sandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini,\nCameron McKinnon, et al. 2022. Constitutional\nai: Harmlessness from ai feedback. arXiv preprint\narXiv:2212.08073.\n\nEmily M. Bender and Alexander Koller. 2020. Climbing\ntowards NLU: On meaning, form, and understanding\nin the age of data. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5185\u20135198, Online. Association for\nComputational Linguistics.\n\n\fYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob\nAndreas, Yoshua Bengio, Joyce Chai, Mirella Lap-\nata, Angeliki Lazaridou, Jonathan May, Aleksandr\nNisnevich, Nicolas Pinto, and Joseph Turian. 2020.\nExperience grounds language. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 8718\u20138735,\nOnline. Association for Computational Linguistics.\n\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\n\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\n\nTyler Burge. 1986. Individualism and psychology. The\n\nPhilosophical Review, 95(1):3\u201345.\n\nHerman Cappelen and Josh Dever. 2021. Making AI\nintelligible: Philosophical foundations. Oxford Uni-\nversity Press.\n\nDavid J Chalmers. 2023. Could a large language model\nbe conscious? arXiv preprint arXiv:2303.07103.\n\nTed Chiang. 2023. Chatgpt is a blurry jpeg of the web.\n\nNew Yorker.\n\nDimitri Coelho Mollo and Rapha\u00ebl Milli\u00e8re. 2023.\narXiv preprint\n\nThe vector grounding problem.\narXiv:2304.01481.\n\nDonald Davidson. 1973. Radical interpretation. Dialec-\n\ntica, pages 313\u2013328.\n\nDonald Davidson. 1986. A coherence theory of truth\nand knowledge. Epistemology: an anthology, pages\n124\u2013133.\n\nDaniel C Dennett. 1971. Intentional systems. The Jour-\n\nnal of Philosophy, 68(4):87\u2013106.\n\nDaniel C Dennett. 1989. The Intentional Stance. MIT\n\npress.\n\nKeith S Donnellan. 1970. Proper names and identifying\n\ndescriptions. Synthese, 21:335\u2013358.\n\nGareth Evans. 1973. The causal theory of names. Pro-\nceedings of the Aristotelian Society, Supplementary\nVolumes, 47:187\u2013225.\n\nMichael C Frank. 2023. Baby steps in evaluating the\ncapacities of large language models. Nature Reviews\nPsychology, 2(8):451\u2013452.\n\nPeter Thomas Geach. 1969. The perils of pauline. The\n\nReview of Metaphysics, pages 287\u2013300.\n\nSimon Goldstein and Cameron Domenico Kirk-\n\nGiannini. manuscript. AI wellbeing.\n\nAlison Gopnik. 2022a. Children, creativity, and the real\n\nkey to intelligence. Observer.\n\nAlison Gopnik. 2022b. What ai still doesn\u2019t know how\n\nto do. The Wall Street Journal.\n\nH Paul Grice. 1957. Meaning. The Philosophical Re-\n\nview, 66(3):377\u2013388.\n\nPeter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zor-\nnitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and\nSrinivasan Iyer. 2023. Methods for measuring, up-\ndating, and visualizing factual beliefs in language\nmodels. In Proceedings of the 17th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics, pages 2714\u20132731, Dubrovnik,\nCroatia. Association for Computational Linguistics.\n\nSaul A Kripke. 1980. Naming and necessity. In Seman-\ntics of natural language, pages 253\u2013355. Springer.\n\nBrenden M Lake and Gregory L Murphy. 2023. Word\nmeaning in minds and machines. Psychological re-\nview, 130(2):401.\n\nKyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy\nKanwisher, Joshua B Tenenbaum, and Evelina Fe-\ndorenko. 2023. Dissociating language and thought\nin large language models: a cognitive perspective.\narXiv preprint arXiv:2301.06627.\n\nMatthew Mandelkern and Tal Linzen. 2023. Do\narXiv preprint\nrefer?\n\nlanguage models\narXiv:2308.05576.\n\nJohn McCarthy. 1979. Ascribing mental qualities to\nmachines. Stanford University. Computer Science\nDepartment.\n\nR. Thomas McCoy, Paul Smolensky, Tal Linzen, Jian-\nfeng Gao, and Asli Celikyilmaz. 2023. How much\ndo language models copy from their training data?\nevaluating linguistic novelty in text generation using\nRAVEN. Transactions of the Association for Compu-\ntational Linguistics, 11:652\u2013670.\n\nMelanie Mitchell and David C Krakauer. 2023. The de-\nbate over understanding in ai\u2019s large language models.\nProceedings of the National Academy of Sciences,\n120(13):e2215907120.\n\nGary Ostertag. 2023. Large language models and exter-\nnalism about reference: Some negative results. (un-\npublished manuscript).\n\nRoma Patel and Ellie Pavlick. 2021. Mapping language\nmodels to grounded conceptual spaces. In Interna-\ntional Conference on Learning Representations.\n\nSteven T Piantadosi and Felix Hill. 2022. Meaning\nwithout reference in large language models. arXiv\npreprint arXiv:2208.02957.\n\n\fHilary Putnam. 1975. The meaning of \u201cmeaning\u201d. In\nMinnesota Studies in the Philosophy of Science, Vol-\nume 7: Language, Mind, and Knowledge, pages 131\u2013\n193. University of Minnesota Press, Minneapolis.\n\nEric Schwitzgebel. 2023. Belief. In Edward N. Zalta\nand Uri Nodelman, editors, The Stanford Encyclope-\ndia of Philosophy, Winter 2023 edition. Metaphysics\nResearch Lab, Stanford University.\n\nCosma Shalizi. 2023. \"Attention\", \"Transformers\", in\n\nNeural Network \"Large Language Models\".\n\nMurray Shanahan, Kyle McDonell, and Laria Reynolds.\n2023. Role play with large language models. Nature,\npages 1\u20136.\n\nEunice Yiu, Eliza Kosoy, and Alison Gopnik. 2023.\nTransmission versus truth, imitation versus inno-\nlarge lan-\nvation: What children can do that\nguage and language-and-vision models cannot\n(yet). Perspectives on Psychological Science, page\n17456916231201401.\n\n\f"}
